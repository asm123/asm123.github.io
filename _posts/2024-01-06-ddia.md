---
layout: post
date: 2024-01-06
title: Book Notes - Designing Data-Intensive Applications
tags: book-notes system-design
last_updated: 2024-04-27
status: budding
---

## Chapter notes

* [Chapter 3: Storage and Retrieval](#chapter-3-storage-and-retrieval)
* [Chapter 7. Transactions](#chapter-7-transactions)

---

## Chapter 3: Storage and Retrieval

### Terminology

* Write amplification: One write to the database resulting in multiple writes to the disk over the course of the database's lifetime.

### B-Trees vs LSM-Trees

#### B-Tree
1. Faster for reads
2. Higher write amplification than LSM-Tree
3. Has to overwrite several pages in the tree
4. Can lead to disk space fragmentatuon
5. Query response time is generally predictable
6. Compaction may not keep up with high write throughput -> unmerged segments on disk increase -> slowed down reads
7. Used by: [MySQL](https://dev.mysql.com/doc/refman/8.0/en/mysql-indexes.html), [PostgreSQL](https://www.postgresql.org/docs/current/btree-implementation.html), [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/19/admin/managing-indexes.html#GUID-2CE1BB91-3EFA-450D-BD31-0C961549F0C2).


#### LSM-Tree
1. Faster for writes
2. Able to sustain higher write throughput due to:
   - lower write amplification
   - sequentially write compact SSTable files
3. Produces smaller files on disk
4. Lower storage overhead with leveled compaction
5. Compaction process can interfere with the performance of ongoing reads and writes -> response time for queries can sometimes be high
6. Each key exists in only one place in index -> can offer strong transaction isolation semantics
7. Used by: [Cassandra](https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlManageOndisk.html), [CockroachDB](https://www.cockroachlabs.com/docs/v23.1/architecture/storage-layer#pebble), [Ressi for Google Spanner](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46103.pdf).

### Other indexing structures

* Secondary index
* Storing values within the index
	* Storing actual row - Clustered index - store indexed row within the index. MySQL InnoDB.
	* Reference to the row stored in a file called *heap file* - unordered, append-only, tracking delete rows
	* Covering index / index with included columns - store some of a table's columns within the index.
* Multi-column index
	* Concatenated index - combine several fields into one key by appending one column to another
* Fuzzy index
	* Full-text search - Lucene
* In-memory database
	* Write a log of changes to disk by writing periodic snapshots to disk or by replicating the in-memory state to other machines.
	* Anti-caching - evicting the least recently used data from memory to disk when there is not enough memory, load back into memory when accessed again in future.
	* NVM - non-volatile memory
	* 


### OLTP and OLAP

* OLTP = online transaction processing = small number of records per query, fetched by key

* OLAP = online analytic processing = aggregate over large number of records

* Data warehouse contains a read-only copy of the data in all the OLTP systems.
	* ETL = Extract-Transform-Load - extract data from OLTP Databases, transform into analysis-friendly schema, clean up, load into data warehouse

* Schemas for analytics
	* Star schema / dimensional modelling
		* Fact table - each row represents an event that occurred at a particular time
		* Dimension table - table for foreign key references to other tables, typically columns in fact tables
	* Snowflake schema - similar to star schema, dimensions are further broken down into subdimensions.
		* More normalized than star schemas.

### Column-oriented storage

* Store all the values from each column together instead of storing all the values form one row together.
* Each column is stored in a separate file.
* Examples: Cassandra, HBase
* Column compression
	* Bitmap encoding - column with n distinct values is condided as n separate bitmaps - one bitmap for each distinct value, one bit for each row.
	* Vectorized processing - bitwise operators on chunks of compressed column data
* Aggregation
	* Materialized views - actual copy of query results with materialized aggregates
	* Data cube / OLAP cube - grid of aggregates grouped by different dimensions


### Further reading

* [Bitcask: A Log-Structured Hash Table for Fast Key/Value Data](https://riak.com/assets/bitcask-intro.pdf)
* [Dremel: Interactive Analysis of Web-Scale Datasets](https://research.google/pubs/pub36632/)
* [LevelDB Implementation Notes](https://github.com/google/leveldb/blob/main/doc/impl.md)
* [Bigtable: A Distributed Storage System for Structured Data](https://research.google/pubs/pub27898/)
* [The Log-Structured Merge-Tree (LSM-Tree)](https://www.cs.umb.edu/~poneil/lsmtree.pdf)
* [Hacking Lucene - The Index Format](https://massiveprogramming.blogspot.com/2014/10/hacking-lucene-index-format-hacker-labs.html)
* [Space/Time Trade-offs in Hash Coding with Allowable Errors](https://people.cs.umass.edu/~emery/classes/cmpsci691st/readings/Misc/p422-bloom.pdf)
* [The Advantages of an LSM vs a B-Tree](http://smalldatum.blogspot.co.uk/2016/01/summary-of-advantages-of-lsm-vs-b-tree.html)
* [Burst Tries: A Fast, Efficient Data Structure for String Keys](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499)
* [Fast String Correction with Levenshtein Automata](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652)
* [Anti-Caching: A New Approach to Database Management System Architecture](http://www.vldb.org/pvldb/vol6/p1942-debrabant.pdf)
* [The Traditional RDBMS Wisdom Is (Almost Certainly) All Wrong](http://slideshot.epfl.ch/talks/166)
* [Dremel made simple with Parquet](https://blog.twitter.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet)
* [The Design and Implementation of Modern Column-Oriented Database Systems](http://cs-www.cs.yale.edu/homes/dna/papers/abadi-column-stores.pdf)

## Chapter 7. Transactions

* A system has to deal with faults in order to be reliable. Implementing fault-tolerance mechanisms is a lot of work.

* Transactions have been the mechanisms of choice for simplifying the fault-tolerance issues. 
  * Conceptually, all the reads and writes in a transaction are executed as one operation: either the transaction succeeds (commit) or it fails (abort, rollback). Application can safely retry if it fails.
  * Safety guarantees: Application is free to ignore certain potential error scenarios and concurrency issues because the database takes care of them.

### Safety guarantees - ACID

* **Atomicity**
  * It is not about concurrency. 
  * Atomicity = abortability = the ability to abort a transaction on error (a fault occurs after some of the writes have been processed) and have all writes from that transaction discarded.
  * Can be implemented using a log for crash recovery.

* **Consistency** 
  * Application-specific notion of a database being in "good state". Application has invariants = certain statements about your data, that must always be true.
  * It is a property of the application and not the database. Database cannot guarantee it. It is the application's responsibility to define the transactions correctly so that they preserve consistency.
  * Not the same as consistency in replica consistency, CAP theorem or consistent hashing.
  
* **Isolation**
  * Concurrently executing transactions are isolated from each other. Each transaction can pretend it is the only transaction running on the database.
  * Formalized as serializability - database ensures that when the concurrent transactions have committed, the result is the same as if they had run one after the another.
  * Serializable isolation is rarely used in practice due to resulting performance penalty.
  * Can be implemented using a lock on each object - allowing only one thread to access an object at any time.

* **Durability**
  * Once a transaction is committed successfully, the data is persisted even in the cases of faults and crashes.
  * Single-node: Data has been written to a nonvolatile storage. Involves a a write-ahead-log for recovery.
  * Replicated: Data has been successfully copied to some number of nodes.

No one technique can provide absolute guarantees. There are only various risk-reduction techniques.

* Multi-object operations are needed when multiple pieces of data need to be kept in sync at once. For example, maintaining referential integrity with foreign keys, updating denormalized information in document data model, updating secondary indexes
* Difficult to implement in distributed datastores across partitions, can get in the way of high availability and performance, error handling without atomicity and lack of isolation can cause concurrency problems.

#### Handling errors and aborts
* ACID databases: If the database is in danger of violating its guarantee of atomicity, isolation or durability, it would rather abandon the transaction entirely than allow it to remain half-finished.
* Datastores with leaderless replication: best-effort basis - db will do as much as it can, and if it runs into an error, it won't undo something it has already done. It is the application's responsibility to recover from errors.
* Retrying an aborted transaction is a simple and effective error handling mechanism, but may not always work to the desired effect. For example, n/w failure after committing but before acknowledging the success of the commit, errors due to overload, permanent errors, transactions with side-effects outside of the database, failure of client process while retrying.

### Weak isolation levels
* Concurrency issues only come into play when one transaction reads data that is concurrently modified by another transaction, or when two transactions try to simultaneously modify the same data.
* Concurrency bugs are hard to find by testing, difficult to reproduce, difficult to reason about in large applications.
* DBs try to hid concurrency issues from application developers by providing transaction isolation.
	* Serializable isolation: db guarantees that concurrent transactions have the same effect as if they ran serially. Has performance cost in practice.
	* Weaker isolation levels: commonly used in practice. Harder to understand and can lead to subtle bugs.
		1. Read committed
		2. Snapshot isolation

**Nonrepeatable read:** When a transaction reads the same row twice and gets different data each time. Example: read skew.

**Phantom read:** A phantom read occurs when, in the course of a transaction, two identical queries are executed, and the collection of rows returned by the second query is different from the first.

Example:
* User A runs the same query twice.
- In between, User B runs a transaction and commits.
- Non-repeatable read: The A row that user A has queried has a different value the second time.
- Phantom read: All the rows in the query have the same value before and after, but different rows are being selected (because B has deleted or inserted some).

Source: [StackOverflow](https://stackoverflow.com/a/11044968).

#### Read Committed
Makes two guarantees:
* **No dirty writes**: while writing to the database, you will only overwrite data that has been committed.
	* Dirty write: Two concurrent transactions try to update the same object in a db, if an earlier write is part of a transaction that has not yet committed, the later write overwrites an uncommitted value.
* **No dirty reads**: while reading from the database, you will only see data that has been committed.
	* Dirty read: A transaction sees uncommitted data from another transaction. May result in a transaction seeing some updates but not others, seeing data that is later rolled back.
	* Any writes by a transaction should only become visible to others when that transaction commits.

Implementation:
* Dirty writes are prevented using row-level locks for writing.
* Dirty reads may be prevented using row-level locks for reading -> transactions will be slow. Commonly implemented as: for every object that is written, database remembers both the old committed value and the new value set by the transaction that currently holds the write lock.

Problems:
* Does not prevent race conditions.
* Read skew is considered acceptable under read committed isolation. Some situations cannot tolerate temporary inconsistencies arising from rad skew - backups, analytic queries and integrity checks.

#### Snapshot isolation and repeatable read
* Each transaction reads from a consistent snapshot of the database.
	* Transaction sees all the data that was committed in the database at the start of the transaction.
	* Even if the data is subsequently changed by another transaction, each transaction sees only the old data from that particular point in time.
	* Readers never block writers, writers never block readers.
* Implemented as *multi-version concurrency control (MVCC)*: Database keeps several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time.
* Read committed uses a separate snapshot for each query, snapshot isolation uses the same snapshot for an entire transaction.
* Visibility rules for observing a consistent snapshot:
	* DB makes a list of all the in-progress transactions at the start of each transaction. Any writes that those transactions have made are ignored even if the transactions subsequently commit.
	* Any writes made by aborted transactions are ignored.
	* Any writes made by transactions with a later transaction id are ignored regardless of whether those transactions have committed.
	* All other writes are visible to the application's queries.
* Indexing:
	* Point to all the versions of an object and require an index query to filter out any object versions that are not visible to the current transaction. Garbage collection removes index entries corresponding to the old object versions.
	* Use an append-only / copy-on-write variant that creates a new copy of modified B-tree page instead of overwriting the page on update. Also requires compaction and garbage collection.

### Preventing lost updates
Lost update problem: An application reads some value from the database, modifies it and writes back the modified value (read-modify-write cycle). If two transactions do this concurrently, one of the modifications can be lost because the second write does not include the first write.

Possible solutions:
* Atomic write operations: `UPDATE counters SET value = value + 1 WHERE key = 'foo';`
	* Remove the need to implement read-modify-write cycle.
	* Usually implemented as *cursor stability* - take an exclusive lock on the object when it is read so that no other transaction can read it until the update as been applied.
	* Alternatively, force all the atomic operation to be executed on a single thread.
* Explicit locking: Application explicitly locks objects that are going to be updated and then performs read-modify-write cycle.
* Automatically detecting lost updates: All read-modify-write cycles to execute in parallel. If the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.
	* DBs can perform the check efficiently in conjunction in snapshot isolation.
* Compare-and-set: Allow an update to happen only if the value has not changed since you last read it. If the current values does not match the previously read value, the update has no effect and the read-modify-write cycle must be retried.
	* May not always prevent lost updates because the read condition may be true even though another concurrent write is occurring.
* Conflict resolution and replication: Replicated databases may have data that is potentially modified concurrently on different nodes. 
	* Common approach is to allow concurrent writes to create several conflicting versions of a value and use application code or special data structures to resolve and merge these versions after the fact. // TODO: examples of such data structures?
	* Commutative atomic operations - apply in different order on different replicas and still get the same result - can work well.
	* Last write wins (LWW) is default in many replicated databases. The conflict resolution is prone to lost updates.

### Write skews and phantoms

* **Write skew**: Two transactions read the same objects and then update some of those objects - different transactions may update different objects. Neither a dirty write nor a lost update if the two transactions are concurrently updating two different objects, but causes a race condition.
	* Ineffective solutions: Atomic single-object operations don't help because multiple objects are involved. Automatic detection and prevention requires true serializable isolation.
	* Possible solutions: Configure constraints for multiple objects with triggers or materialized views. Explicitly lock the rows that the transaction depends on.
* **Phantom**: A write in one transaction changes the result of a search query in another transaction.
	* If there is no object to which we can attach the locks, artificially introduce a lock object into the database.
	* Materializing conflicts: Take a phantom and turn it into a lock conflict on a concrete set of rows that exist in the database.
